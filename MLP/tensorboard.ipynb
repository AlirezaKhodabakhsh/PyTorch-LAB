{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# custom libraries\n",
    "root_main = os.getcwd()\n",
    "os.chdir(\"..\")\n",
    "import TorchCommon as TC\n",
    "os.chdir(root_main)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# main pyperparametrs\n",
    "valid_size=0.2\n",
    "nrm_mean=0.5\n",
    "nrm_std=0.5\n",
    "num_workers=0\n",
    "root_ds = \"D:\\GitHub\\pytorch-lab\\Dataset\" # dataset root\n",
    "root_bm = \"D:\\GitHub\\pytorch-lab\\Best_Models\" # best models root"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# transform\n",
    "trans=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize((nrm_mean,) , (nrm_std,))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#load dataset\n",
    "train_data=datasets.MNIST(root=root_ds,\n",
    "                          train=True, transform=trans, download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#sampler\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# Network\n",
    "class Net(nn.Module):\n",
    "    # Assign Intrinsic Properties of Your Neural Network\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Weights of Layer 1th and Layer 2th Are Intrinsic Properties\n",
    "        self.fc1 = nn.Linear(784, 256, bias=True)\n",
    "        self.fc2 = nn.Linear(256, 100, bias=True)\n",
    "        self.fc3 = nn.Linear(100, 10, bias=True)\n",
    "\n",
    "    # Wiring of Your Network\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu_(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu_(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TensorBoard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a loss and accuracy plot (independently)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Hyperparaameter\n",
    "epochs=5\n",
    "batch_size=64\n",
    "lr=0.1\n",
    "\n",
    "# TrainLoader\n",
    "train_loader=DataLoader(train_data, batch_size= batch_size, num_workers=num_workers, sampler=train_sampler)\n",
    "\n",
    "# Loss and Optimizer\n",
    "model=Net().to(device)\n",
    "criterion =nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# TesnorBoard File\n",
    "root_tb=f'runs/batch {batch_size} LR {lr}'\n",
    "writer = SummaryWriter(root_tb)\n",
    "\n",
    "# Train\n",
    "step=0\n",
    "for epoch in range(1,epochs+1):\n",
    "    for iter_train, (image, label) in enumerate(train_loader, 1):\n",
    "        # preprocess and feedforward process\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(image)\n",
    "        # we want to track loss/acc\n",
    "        loss = criterion(y_hat, label)\n",
    "        acc = np.count_nonzero(label.to('cpu').numpy() == y_hat.argmax(dim=1).to('cpu').numpy())\n",
    "        # updating process\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # -NEW- TensorBoard process\n",
    "        writer.add_scalar(\"Loss Train\", scalar_value= loss, global_step=step)\n",
    "        writer.add_scalar(\"Accuracy Train\", scalar_value= acc, global_step=step)\n",
    "        step+=1\n",
    "        # -NEW- TensorBoard process\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a loss and accuracy plot (in same plot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Hyperparaameter\n",
    "epochs=5\n",
    "batch_size=64\n",
    "lr=0.1\n",
    "\n",
    "# TrainLoader\n",
    "train_loader=DataLoader(train_data, batch_size= batch_size, num_workers=num_workers, sampler=train_sampler)\n",
    "\n",
    "# Loss and Optimizer\n",
    "model=Net().to(device)\n",
    "criterion =nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# TesnorBoard File\n",
    "root_tb=f'runs/batch {batch_size} LR {lr}'\n",
    "writer = SummaryWriter(root_tb)\n",
    "\n",
    "# Train\n",
    "step=0\n",
    "for epoch in range(1,epochs+1):\n",
    "    for iter_train, (image, label) in enumerate(train_loader, 1):\n",
    "        # preprocess and feedforward process\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(image)\n",
    "        # we want to track loss/acc\n",
    "        loss = criterion(y_hat, label)\n",
    "        acc = np.count_nonzero(label.to('cpu').numpy() == y_hat.argmax(dim=1).to('cpu').numpy())\n",
    "        # updating process\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # -NEW- TensorBoard process\n",
    "        writer.add_scalars(\"Loss/Accuracy\", {'Loss' : loss, 'Accuracy': acc}, global_step=step)\n",
    "        step+=1\n",
    "        # -NEW- TensorBoard process\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Histogram"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# Hyperparaameter\n",
    "epochs=5\n",
    "batch_size=64\n",
    "lr=0.1\n",
    "\n",
    "# TrainLoader\n",
    "train_loader=DataLoader(train_data, batch_size= batch_size, num_workers=num_workers, sampler=train_sampler)\n",
    "\n",
    "# Loss and Optimizer\n",
    "model=Net().to(device)\n",
    "criterion =nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# TesnorBoard File\n",
    "root_tb=f'runs/batch {batch_size} LR {lr}'\n",
    "writer = SummaryWriter(root_tb)\n",
    "\n",
    "# Train\n",
    "step=0\n",
    "for epoch in range(1,epochs+1):\n",
    "    for iter_train, (image, label) in enumerate(train_loader, 1):\n",
    "        # preprocess and feedforward process\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(image)\n",
    "        # we want to track loss/acc\n",
    "        loss = criterion(y_hat, label)\n",
    "        acc = np.count_nonzero(label.to('cpu').numpy() == y_hat.argmax(dim=1).to('cpu').numpy())\n",
    "        # updating process\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # -NEW- TensorBoard process\n",
    "        writer.add_histogram(\"FC1_Weights\", model.fc1.weight, global_step=step)\n",
    "        writer.add_histogram(\"FC2_Weights\", model.fc2.weight, global_step=step)\n",
    "        writer.add_histogram(\"FC3_Weights\", model.fc3.weight, global_step=step)\n",
    "        step+=1\n",
    "        # -NEW- TensorBoard process\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# Hyperparaameter\n",
    "batch_size=64\n",
    "\n",
    "# TrainLoader\n",
    "train_loader=DataLoader(train_data, batch_size= batch_size, num_workers=num_workers, sampler=train_sampler)\n",
    "\n",
    "# Loss and Optimizer\n",
    "model=Net()\n",
    "\n",
    "# TesnorBoard File\n",
    "root_tb=f'runs/batch {batch_size}'\n",
    "writer = SummaryWriter(root_tb)\n",
    "\n",
    "# -NEW- TensorBoard process\n",
    "image,label=next(iter(train_loader))\n",
    "writer.add_graph(model, image)\n",
    "# -NEW- TensorBoard process\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### mini-batch image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# Hyperparaameter\n",
    "epochs=1\n",
    "batch_size=61\n",
    "lr=0.1\n",
    "\n",
    "# TrainLoader\n",
    "train_loader=DataLoader(train_data, batch_size= batch_size, num_workers=num_workers, sampler=train_sampler)\n",
    "\n",
    "# Loss and Optimizer\n",
    "model=Net().to(device)\n",
    "criterion =nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# TesnorBoard File\n",
    "root_tb=f'runs/batch {batch_size} LR {lr}'\n",
    "writer = SummaryWriter(root_tb)\n",
    "\n",
    "# Train\n",
    "step=0\n",
    "for epoch in range(1,epochs+1):\n",
    "    for iter_train, (image, label) in enumerate(train_loader, 1):\n",
    "        # preprocess and feedforward process\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(image)\n",
    "        # we want to track loss/acc\n",
    "        loss = criterion(y_hat, label)\n",
    "        acc = np.count_nonzero(label.to('cpu').numpy() == y_hat.argmax(dim=1).to('cpu').numpy())\n",
    "        # updating process\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # -NEW- TensorBoard process\n",
    "        writer.add_images(\"mini=batch data\", image, global_step=iter_train)\n",
    "        step+=1\n",
    "        # -NEW- TensorBoard process\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### mini-batch signal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters Searching"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparaameter\n",
    "epochs=1\n",
    "for batch_size in [10,100,1000]:\n",
    "    for lr in [0.1, 0.01, 0.001]:\n",
    "        # TrainLoader\n",
    "        train_loader=DataLoader(train_data, batch_size= batch_size, num_workers=num_workers, sampler=train_sampler)\n",
    "\n",
    "        # Loss and Optimizer\n",
    "        model=Net().to(device)\n",
    "        criterion =nn.CrossEntropyLoss()\n",
    "        optimizer=optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "        # TesnorBoard File\n",
    "        root_tb=f'runs/batch {batch_size} LR {lr}'\n",
    "        writer = SummaryWriter(root_tb)\n",
    "\n",
    "        # Train\n",
    "        for epoch in range(1,epochs+1):\n",
    "            loss_train, acc_train = TC.train(model,train_loader, device, optimizer, criterion, epoch)\n",
    "\n",
    "        # -NEW- TensorBoard process\n",
    "        writer.add_hparams({'batch_size' : batch_size, 'LR' : lr} ,\n",
    "                           {'Loss train' : loss_train, 'Accuracy train' : acc_train})\n",
    "        # -NEW- TensorBoard process\n",
    "        writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "###### Epoch 1 #############################################\n",
      "############################################################\n",
      "Loss_Train: 2.25\tAcc_Train : 0.14\n",
      "############################################################\n",
      "###### Epoch 1 #############################################\n",
      "############################################################\n",
      "Loss_Train: 0.34\tAcc_Train : 0.89\n",
      "############################################################\n",
      "###### Epoch 1 #############################################\n",
      "############################################################\n",
      "Loss_Train: 0.61\tAcc_Train : 0.83\n",
      "############################################################\n",
      "###### Epoch 1 #############################################\n",
      "############################################################\n",
      "Loss_Train: 1.93\tAcc_Train : 0.47\n",
      "############################################################\n",
      "###### Epoch 1 #############################################\n",
      "############################################################\n",
      "Loss_Train: 0.49\tAcc_Train : 0.84\n",
      "############################################################\n",
      "###### Epoch 1 #############################################\n",
      "############################################################\n",
      "Loss_Train: 1.16\tAcc_Train : 0.72\n",
      "############################################################\n",
      "###### Epoch 1 #############################################\n",
      "############################################################\n",
      "Loss_Train: 2.23\tAcc_Train : 0.26\n",
      "############################################################\n",
      "###### Epoch 1 #############################################\n",
      "############################################################\n",
      "Loss_Train: 2.30\tAcc_Train : 0.10\n",
      "############################################################\n",
      "###### Epoch 1 #############################################\n",
      "############################################################\n",
      "Loss_Train: 1.71\tAcc_Train : 0.51\n",
      "############################################################\n",
      "###### Epoch 1 #############################################\n",
      "############################################################\n",
      "Loss_Train: 2.25\tAcc_Train : 0.22\n",
      "############################################################\n",
      "###### Epoch 1 #############################################\n",
      "############################################################\n",
      "Loss_Train: 2.31\tAcc_Train : 0.12\n",
      "############################################################\n",
      "###### Epoch 1 #############################################\n",
      "############################################################\n",
      "Loss_Train: 2.30\tAcc_Train : 0.12\n"
     ]
    }
   ],
   "source": [
    "epochs=1\n",
    "for batch_size in [2,64,1024]:\n",
    "    for lr in [0.1, 0.01, 0.001]:\n",
    "        train_loader=DataLoader(train_data, batch_size= batch_size, num_workers=num_workers, sampler=train_sampler)\n",
    "        #create model and set loss function and optimizer\n",
    "        model=Net().to(device)\n",
    "        criterion =nn.CrossEntropyLoss()\n",
    "        optimizer=optim.SGD(model.parameters(), lr=lr)\n",
    "        root_tb=f'runs/batch {batch_size} LR {lr}'\n",
    "        writer = SummaryWriter(root_tb)\n",
    "\n",
    "        for epoch in range(1,epochs+1):\n",
    "            loss_train, acc_train = TC.train(model,train_loader, device, optimizer, criterion, epoch)\n",
    "            #loss_valid, acc_valid = TC.valid(model, valid_loader, device, criterion)\n",
    "            #loss_valid_min = TC.save_model(model, optimizer, epoch, root_bm, loss_valid_min, loss_valid)\n",
    "\n",
    "            writer.add_scalar(\"train loss\", scalar_value= loss_train, global_step=epoch)\n",
    "            writer.add_scalar(\"train accuracy\", scalar_value= acc_train, global_step=epoch)\n",
    "            writer.add_hparams({'batch_size' : batch_size, 'LR' : lr} , {'Loss' : loss_train, 'Accuracy' : acc_train})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualizing Dataset Images and Network Weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "###### Epoch 1 #############################################\n",
      "############################################################\n",
      "Loss_Train: 2.26\tAcc_Train : 0.25\n"
     ]
    }
   ],
   "source": [
    "epochs=1\n",
    "#loss_valid_min=np.Inf\n",
    "step=0\n",
    "for batch_size in [64]:\n",
    "    for lr in [0.001]:\n",
    "        train_loader=DataLoader(train_data, batch_size= batch_size, num_workers=num_workers, sampler=train_sampler)\n",
    "        model=Net().to(device)\n",
    "        criterion =nn.CrossEntropyLoss()\n",
    "        optimizer=optim.SGD(model.parameters(), lr=lr)\n",
    "        root_tb=f'runs/batch {batch_size} LR {lr}'\n",
    "        writer = SummaryWriter(root_tb)\n",
    "        for epoch in range(1,epochs+1):\n",
    "            print(60 * \"#\")\n",
    "            print(6 * \"#\" + \" Epoch \" + str(epoch) + \" \" + 45 * \"#\")\n",
    "            print(60 * \"#\")\n",
    "            loss_train, acc_train = 0, 0\n",
    "            model.train()\n",
    "            for iter_train, (image, label) in enumerate(train_loader, 1):\n",
    "                image, label = image.to(device), label.to(device)\n",
    "\n",
    "                img_grid=torchvision.utils.make_grid(image)\n",
    "                writer.add_image(\"mnist image\", img_grid)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                y_hat = model.(image)\n",
    "                loss = criterion(y_hat, label)\n",
    "                acc = np.count_nonzero(label.to('cpu').numpy() == y_hat.argmax(dim=1).to('cpu').numpy())\n",
    "                loss_train += loss.item() * image.size(0)\n",
    "                acc_train += acc\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                writer.add_scalar(\"Training Loss\", loss, global_step=step)\n",
    "                writer.add_scalar(\"Training Acc\", acc, global_step=step)\n",
    "                writer.add_histogram(\"Weights Layer One\", model.fc1.weight, global_step=step)\n",
    "                step+=1\n",
    "\n",
    "            loss_train /= len(train_loader.sampler)\n",
    "            acc_train /= len(train_loader.sampler)\n",
    "            print(\"Loss_Train: {:.2f}\\tAcc_Train : {:.2f}\".format(loss_train, acc_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "60000"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "batch_size=200\n",
    "train_loader=DataLoader(train_data, batch_size= batch_size, num_workers=num_workers, sampler=train_sampler)\n",
    "writer = SummaryWriter(f\"runs\")\n",
    "classes=['0','1','2','3','4','5','6','7','8','9']\n",
    "for image, label in train_loader:\n",
    "    x=image\n",
    "    y=label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "features= x.flatten(start_dim=1)\n",
    "class_labels= [ classes[i] for i in y ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "writer.add_embedding(mat = features,\n",
    "                 metadata =  class_labels,\n",
    "                 label_img=x,\n",
    "                     global_step=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensorboard Embedding Projector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "# helper function\n",
    "writer = SummaryWriter(f\"runs\")\n",
    "classes=['0','1','2','3','4','5','6','7','8','9']\n",
    "def select_n_random(data, labels, n=1000):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(train_data.data, train_data.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_19692/2810002506.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mimages\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "images.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}